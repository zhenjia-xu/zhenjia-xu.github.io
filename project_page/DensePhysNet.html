<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" class="gr__psd_csail_mit_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

    <script src="./DensePhysNet_files/head.js"></script>        <meta name="viewport" content="width=device-width, initial-scale=1">    <link rel="shortcut icon" href="http://psd.csail.mit.edu/images/favicon.ico">
    <meta name="description" content="DensePhysNet">
    <meta name="keywords" content="Vision,Intuitive Physics,Learning,Computer Science,Cognitive Science,Robotics">

    <title>DensePhysNet</title>
    <link rel="stylesheet" href="./DensePhysNet_files/font.css">
    <link rel="stylesheet" href="./DensePhysNet_files/main.css">

  </head>

  <body data-gr-c-s-loaded="true">

    <div class="outercontainer">
      <div class="container">

        <div class="content project_title">
          <h1>DensePhysNet: Learning Dense Physical Object Representations via Multi-step Dynamic Interactions</h1>
          <br>
        </div>

        <div class="content">
            <div class="text">
              <h3>Abstract</h3>
              <p>We study the problem of learning physical object representations for robot manipulation. Understanding object physics is critical for successful object manipulation, but also challenging because physical object properties can rarely be inferred from the objectâ€™s static appearance. In this paper, we propose DensePhysNet, a system that actively executes a sequence of dynamic interactions (e.g., sliding and colliding), and uses a deep predictive model over its visual observations to learn dense, pixel-wise representations that reflect the physical properties of observed objects. Our experiments in both simulation and real settings demonstrate that the learned representations carry rich physical information, and can directly be used to decode physical object properties such as friction and mass. The use of dense representation enables DensePhysNet to generalize well to novel scenes with more objects than in training. With knowledge of object physics, the learned representation also leads to more accurate and efficient manipulation in downstream tasks than the state-of-the-art.</p>
            </div>
          </div>
  
          <div class="content">
            <div class="text">
              <h3>Publication</h3>
              <ul>
                <li>
                <div class="title"><a name="DensePhysNet_RSS">DensePhysNet: Learning Dense Physical Object Representations via Multi-step Dynamic Interactions</a></div>
                <div class="authors">
                  <a href="http://zhenjiaxu.com/">Zhenjia Xu</a>,
                  <a href="http://jiajunwu.com/">Jiajun Wu</a>,
                  <a href="http://andyzeng.github.io/">Andy Zeng</a>,
                  <a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>, and
                  <a href="http://shurans.github.io/">Shuran Song</a>
                </div>
                <div>
                  <span class="venue">RSS 2019</span>
                  <span class="tag"><a href="http://zhenjiaxu.com/paper/DensePhysNet_RSS.pdf">Paper</a></span>
                  <span class="tag"><a href="http://zhenjiaxu.com/bibtex/DensePhysNet_RSS.bib">BibTeX</a></span>
                </div>
                </li>
              </ul>
            </div>
          </div>

        <div class="content project_headline">
          <div class="img" style="text-align:center;max-width:500px">
            <img class="img_responsive" src="./DensePhysNet_files/DensePhysNet.png" alt="DensePhysnet">
          </div>
          <br>
          <div class="text">
            <p>Figure 1: Our goal is to build a robotic system that learns a dense physical object representation from a few dynamic interactions with objects. The learned representation can then be used to decode object properties such as its material and mass, applied in manipulation tasks such as sliding objects with unknown physics, and combined with a physics engine to tackle novel tasks.</p>
          </div>
        </div>

        <div class="content project_headline">
          <div class="img" style="text-align:center;max-width:1000px">
            <img class="img_responsive" src="./DensePhysNet_files/model.png" alt="DensePhysnet_model">
          </div>
          <br>
          <div class="text">
            <p>Figure 2: DensePhysNet consists of five modules: (a) an image encoder, (b) a multi-step information aggregator, (c) an action encoder, (d) a cross convolutional layer, and (e) a motion predictor.</p>
          </div>
        </div>

        <div class="content project_headline">
          <!-- <iframe width="720" height="405" aria-label="YouTube Video, Supplementary Material of ICLR Submission" src="./Parts, Structure, and Dynamics_files/LbUZo6ocDNI.html" frameborder="0" allowfullscreen=""></iframe> -->
          
          <video src="DensePhysNet_files/demo.m4v" width="800" controls="controls">
          </video>
          <div class="text">
            <p>Supplementary video for our setup, model, and results</p>
          </div>
        </div>

        <!-- <div class="content">
          <div class="text">
            <h3>Related Publications</h3>
            <ul>
              <li>
              <div class="title"><a name="galileo_nips">Push-Net: Deep Planar Pushing for Objects with Unknown Physical Properties</a></div>
              <div class="authors">
                <a>Jue Kun Li</a>,
                <a href="https://www.comp.nus.edu.sg/~dyhsu/">David Hsu</a>, and
                <a href="https://www.comp.nus.edu.sg/~leews/">Wee Sun Lee</a>
              </div>
              <div>
                <span class="venue">RSS 2018</span>
                <span class="tag"><a href="http://www.roboticsproceedings.org/rss14/p24.pdf">Paper</a></span>
              </div>
              </li>
            </ul>
            
          </div>
        </div> -->

      </div>
    </div>

  

</body></html>